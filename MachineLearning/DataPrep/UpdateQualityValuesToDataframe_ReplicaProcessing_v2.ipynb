{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "micro-textbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:11:47.889512Z",
     "start_time": "2021-08-03T15:11:47.785582Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions import * \n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "favorite-daniel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:11:51.903797Z",
     "start_time": "2021-08-03T15:11:51.800305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a handle for the record_path\n",
    "record_path = '../Scripts&Records/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "further-facial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:11:55.503139Z",
     "start_time": "2021-08-03T15:11:55.396677Z"
    }
   },
   "outputs": [],
   "source": [
    "def progress_bar(id_,array_):\n",
    "    # Progress Bar\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Processing file {id_+1} of {len(array_)} files... {round(100*(id_+1)/len(array_),2)}% Complete\")\n",
    "    return\n",
    "\n",
    "\n",
    "def status_update_msg(msg):\n",
    "    clear_output(wait=True)\n",
    "    print(msg)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "printable-manhattan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:13:01.122765Z",
     "start_time": "2021-08-03T15:13:00.458247Z"
    }
   },
   "outputs": [],
   "source": [
    "express_hist20 = pd.read_csv(record_path+'express_hist20.csv',index_col=[0])\n",
    "pMain_hist20 = pd.read_csv(record_path+'pMain_hist20.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "exclusive-reunion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:13:02.100310Z",
     "start_time": "2021-08-03T15:13:01.992806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90090, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "express_hist20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "increasing-bookmark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:13:02.683011Z",
     "start_time": "2021-08-03T15:13:02.578042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90090, 5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pMain_hist20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "virgin-region",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:13:03.298143Z",
     "start_time": "2021-08-03T15:13:03.188303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    90090\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "express_hist20['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "photographic-batch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:13:04.032976Z",
     "start_time": "2021-08-03T15:13:03.921875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    89457\n",
       "1      633\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pMain_hist20['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deadly-retailer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T15:13:06.519554Z",
     "start_time": "2021-08-03T15:13:06.197814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example scale_cnvrt_dic to convert hitcoords from cern coordinates to bin coordinates (-1.8,1-.8)->(31,50)\n",
      "{'x_0': -4.9, 'x_1': -4.8, 'x_2': -4.7, 'x_3': -4.6, 'x_4': -4.5, 'x_5': -4.4, 'x_6': -4.3, 'x_7': -4.2, 'x_8': -4.1, 'x_9': -4.0, 'x_10': -3.9, 'x_11': -3.8, 'x_12': -3.7, 'x_13': -3.6, 'x_14': -3.5, 'x_15': -3.4, 'x_16': -3.3, 'x_17': -3.2, 'x_18': -3.1, 'x_19': -3.0, 'x_20': -2.9, 'x_21': -2.8, 'x_22': -2.7, 'x_23': -2.6, 'x_24': -2.5, 'x_25': -2.4, 'x_26': -2.3, 'x_27': -2.2, 'x_28': -2.1, 'x_29': -2.0, 'x_30': -1.9, 'x_31': -1.8, 'x_32': -1.7, 'x_33': -1.6, 'x_34': -1.5, 'x_35': -1.4, 'x_36': -1.3, 'x_37': -1.2, 'x_38': -1.1, 'x_39': -1.0, 'x_40': -0.9, 'x_41': -0.8, 'x_42': -0.7, 'x_43': -0.6, 'x_44': -0.5, 'x_45': -0.4, 'x_46': -0.3, 'x_47': -0.2, 'x_48': -0.1, 'x_49': 0.0, 'x_50': 0.1, 'x_51': 0.2, 'x_52': 0.3, 'x_53': 0.4, 'x_54': 0.5, 'x_55': 0.6, 'x_56': 0.7, 'x_57': 0.8, 'x_58': 0.9, 'x_59': 1.0, 'x_60': 1.1, 'x_61': 1.2, 'x_62': 1.3, 'x_63': 1.4, 'x_64': 1.5, 'x_65': 1.6, 'x_66': 1.7, 'x_67': 1.8, 'x_68': 1.9, 'x_69': 2.0, 'x_70': 2.1, 'x_71': 2.2, 'x_72': 2.3, 'x_73': 2.4, 'x_74': 2.5, 'x_75': 2.6, 'x_76': 2.7, 'x_77': 2.8, 'x_78': 2.9, 'x_79': 3.0, 'x_80': 3.1, 'x_81': 3.2, 'x_82': 3.3, 'x_83': 3.4, 'x_84': 3.5, 'x_85': 3.6, 'x_86': 3.7, 'x_87': 3.8, 'x_88': 3.9, 'x_89': 4.0, 'x_90': 4.1, 'x_91': 4.2, 'x_92': 4.3, 'x_93': 4.4, 'x_94': 4.5, 'x_95': 4.6, 'x_96': 4.7, 'x_97': 4.8, 'x_98': 4.9}\n",
      "Example hitstring: 'Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:6829.0'\n",
      "Example tenths_ceil(-1.850,1.723): (-1.8,1.8)\n",
      "Example of transform_hitstring: (-1.8, 1.8, 'Y', '0', 6829)\n",
      "Example prep_quality_feature for pMain_hist20 dataframe, hist_index=0, txt_file_path = ... :\n",
      "run_363710/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1\n",
      "???: run_363710-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error: the unique histogram chosen as hist_index cannot be found for any of the hist_paths in val_list'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Works as expected\n",
    "def scale_cnvrt_dic(hists_df,index_of_hist_of_interest,x_or_y_axis_as_0or1):\n",
    "    \"\"\"\n",
    "    # So, for example, using the pMain set of 20 histograms we have compiled, to convert the 0th histogram's x(as 0) axis...do the following \n",
    "    scale_cnvrt_dic(pMain_hist20,0,0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting up to convert the scale from the bin numbers (0-98, example only) to the dqm's histogram's scale values (-4.9 to 4.9, example only)\n",
    "    tmp = hists_df[hists_df['paths']==hists_df['paths'].unique()[index_of_hist_of_interest]]\n",
    "    tmp_i = np.array([(idx) for idx,i in enumerate(range(int(tmp[tmp.columns[x_or_y_axis_as_0or1+1]].values.max()+1)))])\n",
    "    tmp_int = np.interp(tmp_i,(tmp_i.min(),tmp_i.max()),(-tmp[tmp.columns[x_or_y_axis_as_0or1+1]].values.max()/20,tmp[tmp.columns[x_or_y_axis_as_0or1+1]].values.max()/20))\n",
    "    tmp_int = tmp_int.round(2)\n",
    "    \n",
    "    # Prepare the conversion dictionary for this histogram ['bin_coordinate':dqm_scale_value]\n",
    "    dict_convertX = {}\n",
    "    for idx,val in enumerate(tmp_i):\n",
    "        for idxx,vall in enumerate(tmp_int):\n",
    "            if idx==idxx:\n",
    "                if x_or_y_axis_as_0or1 == 0:\n",
    "                    dict_convertX['x_'+str(val)] = vall\n",
    "                else:\n",
    "                    dict_convertX['y_'+str(val)] = vall\n",
    "\n",
    "    return dict_convertX\n",
    "\n",
    "#Example\n",
    "print('Example scale_cnvrt_dic to convert hitcoords from cern coordinates to bin coordinates (-1.8,1-.8)->(31,50)')\n",
    "print(scale_cnvrt_dic(pMain_hist20,0,0))\n",
    "\n",
    "print(f\"Example hitstring: 'Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:6829.0'\")\n",
    "\n",
    "def tenths_ceil(num_str):\n",
    "    \"\"\"\n",
    "    Currently, this gives the correct value when transforming (eta,phi)_cern coordinates to (eta,phi)_bin(the dataframe version of histograms) coordinates.\n",
    "    \n",
    "    This may not work in all cases, double check to make sure???\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get num_str as string\n",
    "    num_str = str(num_str)\n",
    "    \n",
    "    # Get num_str_fixed as num_str with the '-' removed if it exists\n",
    "    num_str_fixed = num_str.replace('-','')\n",
    "    \n",
    "    # Create the tuples of ints and decimals\n",
    "    int_tups = [('10e'+str(idx),char) for idx,char in enumerate(num_str_fixed.split(\".\")[0][::-1])]\n",
    "    dec_tups = [('10e'+str(-1*idx-1),char) for idx,char in enumerate(num_str_fixed.split(\".\")[1])]\n",
    "    \n",
    "    # Loop through decimal tuples\n",
    "    for id_,tup in enumerate(dec_tups):\n",
    "        \n",
    "        # Loop through each individual tuple's decimal value as it loops through tuples in decimals\n",
    "        for dec in tup[1]:\n",
    "            \n",
    "            # if its the first decimal, skip it\n",
    "            if id_ == 0:\n",
    "                continue\n",
    "                \n",
    "            # if any other decimals are greater than 0\n",
    "            if int(dec) > 0:\n",
    "                \n",
    "                # and if the given num_str input is negative, return the negative value truncated to the tenths\n",
    "                if '-' in num_str:  \n",
    "                    return -1 * ( float( ''.join( [tup[1] for tup in int_tups[::-1]] ) ) + float( '0.'+str ( int(dec_tups[0][1]) ) ) )\n",
    "                \n",
    "                # else if its not negative, return the postive value's ceiling to the tenths\n",
    "                else:\n",
    "                    return  float( ''.join( [tup[1] for tup in int_tups[::-1]] ) ) + float( '0.'+str ( int(dec_tups[0][1])+1 ) )\n",
    "                \n",
    "    # if none of the previous loops return a value, the num_str was not rounded up, so return the same negative value\n",
    "    if '-' in num_str:\n",
    "        return -1* ( float(''.join([tup[1] for tup in int_tups[::-1]]))+float('0.'+dec_tups[0][1]) )\n",
    "    \n",
    "    # else if none of the previous loops return a value, the num_str was not rounded up, so return the same positive value\n",
    "    else:        \n",
    "        return float(''.join([tup[1] for tup in int_tups[::-1]]))+float('0.'+dec_tups[0][1])\n",
    "\n",
    "print(f'Example tenths_ceil(-1.850,1.723): ({tenths_ceil(-1.850)},{tenths_ceil(1.723)})')\n",
    "    \n",
    "    \n",
    "def transform_hitstring(line):\n",
    "    \"\"\"\n",
    "    Takes a line in as the format received from the txt_file which was copy/pasted from cern's dqm display on a specific histogram, then extracts the (x_hitcoord,y_hitcoord)\n",
    "    \"\"\"\n",
    "    #line = \"Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:\t6829.0\"\n",
    "    \n",
    "    color_identifier_string=line[0]\n",
    "    \n",
    "    hit_number = line.replace(line[0],'').split('-',1)[0]\n",
    "    \n",
    "    occ_val = int(line.split(':')[1].replace('\\t','').split('.')[0])\n",
    "    \n",
    "    # Begin the process of extracting the x_hitcoord and y_hitcoord from line\n",
    "    line = line.replace(color_identifier_string+hit_number+'-'+'(eta,phi)[OSRatio]=','')\n",
    "    line = line.split(')')[0]\n",
    "    line = line.replace(\"(\",'')\n",
    "    line = line.split(',')\n",
    "    x_hitcoord, y_hitcoord = tenths_ceil( float( line[0] ) ), tenths_ceil( float ( line[1] ) )\n",
    "    \n",
    "    return ( x_hitcoord, y_hitcoord, color_identifier_string, hit_number, occ_val )\n",
    "\n",
    "# Example string, works as expected\n",
    "print(f'Example of transform_hitstring: {transform_hitstring(\"Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:\t6829.0\")}')\n",
    "\n",
    "def extract_val_list(txt_file_path):\n",
    "    \"\"\"\n",
    "        \n",
    "    txt_file_path requires the pathname with the filename and file extension included.\n",
    "    Example: \"dir1/dir2/dir3/filename.txt\"\n",
    "    NOTE: Our current txt_file_path working directory is: \"../hist_targets_txt_files/\" such that the text file would look like \"../hist_targets_txt_files/filename.txt\"\n",
    "    \n",
    "    txt_file Structure:\n",
    "    It is a .txt file structured from a paste function after copying from the dqm starting from the line that reads \"NRedBins:\", then \"NYellowBins:\", then the lines of most interest.\n",
    "    From there, the lines of interest proceed line by line in a format such as the following ...\n",
    "    C#-(eta,phi)[OSRatio]=(x_coord,y_coord)[7.59e+01]: occ_val \n",
    "    where\n",
    "    C = color character identifier (Y for yellow bin, R for red bin)\n",
    "    # = the number of the R/Y bin (if there are NYellowBins=44 then this number will be a number from 0-44)\n",
    "    x_coord = the eta or x coordinate location of the point of interest (red or yellow hit)\n",
    "    y_coord = the phi or y coordinate location of the point of interest (red or yellow hit)\n",
    "    occ_Val = the occupancy value that was recorded at the location (x_coord,y_coord) \n",
    "    \n",
    "    val_list Structure:\n",
    "    It is a list of tuples whose tuples are each (x_hitcoord,y_hitcoord) such that ...\n",
    "    val_list = [ (xhc_0,yhc_0), (xhc_1,yhc_1), ..., (xhc_n,yhc_n) ]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Initialize the val_list\n",
    "    val_list = []\n",
    "    \n",
    "       \n",
    "    # Get a handle for the hist_path based on the txt_file_path ???\n",
    "    hist_path = txt_file_path.replace('-','/').replace('.txt','')\n",
    "    print(hist_path)\n",
    "    \n",
    "    # Open the text file\n",
    "    # directory_path = '../hist_targets_txt_files/'\n",
    "    with open('../hist_targets_txt_files/'+txt_file_path,\"r\") as f:       \n",
    "        # Extract the path_name(first line of the txt_file) - the first line is the path_name\n",
    "        path_name = f.readline()\n",
    "        \n",
    "        # Read the text file line by line and\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            # Skip these two lines\n",
    "            if 'NRedBins' in line:\n",
    "                continue\n",
    "            if 'NYellowBins' in line:\n",
    "                continue\n",
    "            \n",
    "            # Get a handle for the transform of the hitstring in each line of txt_file_path to the tuple of values of interest\n",
    "            transformed_line = transform_hitstring(line)\n",
    "            \n",
    "            # (x_hitcoord, y_hitcoord, color_identifier_string, hit_number, occ_val, hist_path)\n",
    "            val_list.append( ( transformed_line[0], transformed_line[1], transformed_line[2], transformed_line[3], transformed_line[4], hist_path ) )\n",
    "    \n",
    "    # return the val_list in the format as described above\n",
    "    return val_list\n",
    "\n",
    "# This example is very large and very long, turn on for debugging\n",
    "# print(f'Example extract_val_list using txt_file_path=\"run_348251-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt:')\n",
    "# display(extract_val_list(\"run_348251-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\"))\n",
    "    \n",
    "    \n",
    "\n",
    "def prep_quality_feature(list_of_hists_df, hist_index, txt_file_path):\n",
    "        \n",
    "    \"\"\"\n",
    "    val_list_xy must be structured as follows - it is a list of tuples whose tuples are each (x_hitcoord,y_hitcoord) such that \n",
    "    val_list_xy = [ (xhc_0,yhc_0), (xhc_1,yhc_1), ..., (xhc_n,yhc_n) ]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Convert the txt_file_path with '-'s in it to txt_file_path with '/'s in it instead\n",
    "#     txt_file_path2 = txt_file_path.replace('-','/')\n",
    "    \n",
    "    # Extract and get a handle for val_list from txt_file_path\n",
    "    val_list = extract_val_list(txt_file_path)\n",
    "    \n",
    "    # Convert our val_list tuple of 5 values to a tuple of 2 coordinate values (x,y)\n",
    "    val_list_xy = [(tup[0],tup[1]) for tup in val_list]\n",
    "    \n",
    "    # If the histogram of interest is in the list of paths that are associated with specific red/yellow hit coordinates (hit_n = (x,y,color,occ,hist_path))\n",
    "    print('???:',txt_file_path)\n",
    "    \n",
    "    if list_of_hists_df['paths'].unique()[hist_index] in [tup[5] for tup in val_list]:\n",
    "        \n",
    "        # Get a handle for the histogram we are constructing the quality feature for\n",
    "        tmp = list_of_hists_df[list_of_hists_df['paths']==list_of_hists_df['paths'].unique()[hist_index]]\n",
    "    else:\n",
    "        return \"Error: the unique histogram chosen as hist_index cannot be found for any of the hist_paths in val_list\"\n",
    "    \n",
    "    # Get the coordinate conversion dictionary\n",
    "    cnvrt_dic_x, cnvrt_dic_y = scale_cnvrt_dic(list_of_hists_df,hist_index,0), scale_cnvrt_dic(list_of_hists_df,hist_index,1)\n",
    "    \n",
    "    # Loop through the quality values, if the coordinates match the location of the hit, modify their quality value\n",
    "    for idx,val in enumerate(tmp['quality'].values):\n",
    "        \n",
    "        # If the tuple (x,y) from histogram tmp is in the list of (x,y) tuples from the hit value list (val_list_xy)\n",
    "        if (cnvrt_dic_x['x_'+str(int(tmp.iloc[idx,1]))],cnvrt_dic_y['y_'+str(int(tmp.iloc[idx,2]))]) in val_list_xy:\n",
    "            # Set the quality class for this hit (0/1 for green/red, 0/1/2 for green/yellow/red ...for now we just use 0/1)\n",
    "            tmp.iloc[idx,4] = 1 \n",
    "    \n",
    "    # Return the list_of_hists_df whose 'quality' values have been updated\n",
    "    return tmp\n",
    "\n",
    "# Example prep_quality_feature\n",
    "print(f'Example prep_quality_feature for pMain_hist20 dataframe, hist_index=0, txt_file_path = ... :')\n",
    "display(prep_quality_feature(pMain_hist20, 0, \"run_363710-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tested-import",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T15:44:31.282438Z",
     "start_time": "2021-08-02T15:44:29.672308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_348251-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls ../hist_targets_txt_files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-mitchell",
   "metadata": {},
   "source": [
    "### Main Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-authentication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T15:24:36.691998Z",
     "start_time": "2021-08-02T15:24:36.584875Z"
    }
   },
   "source": [
    "1. Find the hist_index of the histogram you are interested in adding quality values to, write this run number and stream information down (find ftag if necessary if you dont remember), and find it in the dqm display to prepare the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "secure-majority",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:54:42.118394Z",
     "start_time": "2021-08-02T23:54:41.974742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['run_348251/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363664/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363710/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363738/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363830/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363910/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363947/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363979/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364030/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364076/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364098/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364160/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364214/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364292/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "express_hist20['paths'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imperial-first",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:54:43.220195Z",
     "start_time": "2021-08-02T23:54:43.077715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['run_363710/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363738/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363830/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363910/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363947/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363979/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364030/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364076/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364098/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364160/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364214/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364292/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_348251/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363664/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pMain_hist20['paths'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-cigarette",
   "metadata": {},
   "source": [
    "2. With the run, stream, and ftag in hand, go to the web display for this particular histogram \"<run_######>/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1\", and copy paste the red/yellow values in the text file as was done with the first text file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-banana",
   "metadata": {},
   "source": [
    "3. Add the run path information(at the top of the dqm display histogram view as the first line to the text file as done previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-batch",
   "metadata": {},
   "source": [
    "4. update the .txt file path in the below block, the appropriate hist_index number, and the appropriate dataframes_hist variable in the prep_quality_Feature function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-mother",
   "metadata": {},
   "source": [
    "5. concatenate as shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-american",
   "metadata": {},
   "source": [
    "6. Test to make sure 44+new values exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "instant-consensus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:56:46.017017Z",
     "start_time": "2021-08-02T23:56:45.885060Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3e1f2140d8ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp2' is not defined"
     ]
    }
   ],
   "source": [
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "healthy-middle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:56:55.614753Z",
     "start_time": "2021-08-02T23:56:54.514119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_363710/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1\n",
      "???: run_363710-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Get a subset that does not include unique()[hist_index] == 0\n",
    "tmp = pMain_hist20[pMain_hist20['paths']!=pMain_hist20['paths'].unique()[0]]\n",
    "# Prepare the modified quality values\n",
    "tmp2 = prep_quality_feature(pMain_hist20, 0, \"run_363710-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\")\n",
    "# then concatonate the pMain_hist20 with tmp_df\n",
    "pMain_hist20 = pd.concat([tmp,tmp2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interior-tomorrow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:56:58.766472Z",
     "start_time": "2021-08-02T23:56:58.623401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['run_363738/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363830/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363910/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363947/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363979/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364030/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364076/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364098/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364160/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364214/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364292/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_348251/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363664/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363710/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pMain_hist20['paths'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "entitled-oxide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T00:08:10.413639Z",
     "start_time": "2021-08-03T00:08:10.299141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    89654\n",
       "1      436\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pMain_hist20['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comparable-genius",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:59:25.813642Z",
     "start_time": "2021-08-02T23:59:25.691227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48396048396048397"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*pMain_hist20['quality'].value_counts().values[1]/(pMain_hist20['quality'].value_counts().values[0]+pMain_hist20['quality'].value_counts().values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-exploration",
   "metadata": {},
   "source": [
    "SO im getting about .2% per 200 datapoint document..ouch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-tuning",
   "metadata": {},
   "source": [
    "There should be 363 + 44 = 407. This is not the number we are getting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-painting",
   "metadata": {},
   "source": [
    "However, it is possible that the dqm display is not printing out all 300+ values. That is what appears to be happening.\n",
    "- 200 through 363\n",
    "are missing specifically\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "iraqi-isaac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T00:08:14.052363Z",
     "start_time": "2021-08-03T00:08:12.450618Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the databases\n",
    "express_hist20.to_csv(record_path+'express_hist20.csv')\n",
    "pMain_hist20.to_csv(record_path+'pMain_hist20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-montreal",
   "metadata": {},
   "source": [
    "<hr style='border:solid 1px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import * \n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "brazilian-donor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T16:22:33.886699Z",
     "start_time": "2021-08-03T16:22:32.611270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example scale_cnvrt_dic to convert hitcoords from cern coordinates to bin coordinates (-1.8,1-.8)->(31,50)\n",
      "{'x_0': -4.9, 'x_1': -4.8, 'x_2': -4.7, 'x_3': -4.6, 'x_4': -4.5, 'x_5': -4.4, 'x_6': -4.3, 'x_7': -4.2, 'x_8': -4.1, 'x_9': -4.0, 'x_10': -3.9, 'x_11': -3.8, 'x_12': -3.7, 'x_13': -3.6, 'x_14': -3.5, 'x_15': -3.4, 'x_16': -3.3, 'x_17': -3.2, 'x_18': -3.1, 'x_19': -3.0, 'x_20': -2.9, 'x_21': -2.8, 'x_22': -2.7, 'x_23': -2.6, 'x_24': -2.5, 'x_25': -2.4, 'x_26': -2.3, 'x_27': -2.2, 'x_28': -2.1, 'x_29': -2.0, 'x_30': -1.9, 'x_31': -1.8, 'x_32': -1.7, 'x_33': -1.6, 'x_34': -1.5, 'x_35': -1.4, 'x_36': -1.3, 'x_37': -1.2, 'x_38': -1.1, 'x_39': -1.0, 'x_40': -0.9, 'x_41': -0.8, 'x_42': -0.7, 'x_43': -0.6, 'x_44': -0.5, 'x_45': -0.4, 'x_46': -0.3, 'x_47': -0.2, 'x_48': -0.1, 'x_49': 0.0, 'x_50': 0.1, 'x_51': 0.2, 'x_52': 0.3, 'x_53': 0.4, 'x_54': 0.5, 'x_55': 0.6, 'x_56': 0.7, 'x_57': 0.8, 'x_58': 0.9, 'x_59': 1.0, 'x_60': 1.1, 'x_61': 1.2, 'x_62': 1.3, 'x_63': 1.4, 'x_64': 1.5, 'x_65': 1.6, 'x_66': 1.7, 'x_67': 1.8, 'x_68': 1.9, 'x_69': 2.0, 'x_70': 2.1, 'x_71': 2.2, 'x_72': 2.3, 'x_73': 2.4, 'x_74': 2.5, 'x_75': 2.6, 'x_76': 2.7, 'x_77': 2.8, 'x_78': 2.9, 'x_79': 3.0, 'x_80': 3.1, 'x_81': 3.2, 'x_82': 3.3, 'x_83': 3.4, 'x_84': 3.5, 'x_85': 3.6, 'x_86': 3.7, 'x_87': 3.8, 'x_88': 3.9, 'x_89': 4.0, 'x_90': 4.1, 'x_91': 4.2, 'x_92': 4.3, 'x_93': 4.4, 'x_94': 4.5, 'x_95': 4.6, 'x_96': 4.7, 'x_97': 4.8, 'x_98': 4.9}\n",
      "Example hitstring: 'Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:6829.0'\n",
      "Example tenths_ceil(-1.850,1.723): (-1.8,1.8)\n",
      "Example of transform_hitstring: (-1.8, 1.8, 'Y', '0', 6829)\n",
      "Example prep_quality_feature for pMain_hist20 dataframe, hist_index=0, txt_file_path = ... :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>occ</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14074</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14075</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14076</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14077</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14078</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20504</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>98</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20505</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>98</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20506</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20507</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>98</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20508</th>\n",
       "      <td>run_363710/CaloMonitoring/ClusterMon/CaloCalTo...</td>\n",
       "      <td>98</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6435 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   paths   x   y  occ  quality\n",
       "14074  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...   0   0    0        0\n",
       "14075  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...   0   1    0        0\n",
       "14076  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...   0   2    0        0\n",
       "14077  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...   0   3    0        0\n",
       "14078  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...   0   4    0        0\n",
       "...                                                  ...  ..  ..  ...      ...\n",
       "20504  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...  98  60    0        0\n",
       "20505  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...  98  61    0        0\n",
       "20506  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...  98  62    0        0\n",
       "20507  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...  98  63    0        0\n",
       "20508  run_363710/CaloMonitoring/ClusterMon/CaloCalTo...  98  64    0        0\n",
       "\n",
       "[6435 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a handle for the record_path\n",
    "record_path = '../Scripts&Records/'\n",
    "\n",
    "def progress_bar(id_,array_):\n",
    "    # Progress Bar\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Processing file {id_+1} of {len(array_)} files... {round(100*(id_+1)/len(array_),2)}% Complete\")\n",
    "    return\n",
    "\n",
    "\n",
    "def status_update_msg(msg):\n",
    "    clear_output(wait=True)\n",
    "    print(msg)\n",
    "    return\n",
    "\n",
    "\n",
    "express_hist20 = pd.read_csv(record_path+'express_hist20.csv',index_col=[0])\n",
    "pMain_hist20 = pd.read_csv(record_path+'pMain_hist20.csv',index_col=[0])\n",
    "\n",
    "\n",
    "# Works as expected\n",
    "def scale_cnvrt_dic(hists_df,index_of_hist_of_interest,x_or_y_axis_as_0or1):\n",
    "    \"\"\"\n",
    "    # So, for example, using the pMain set of 20 histograms we have compiled, to convert the 0th histogram's x(as 0) axis...do the following \n",
    "    scale_cnvrt_dic(pMain_hist20,0,0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting up to convert the scale from the bin numbers (0-98, example only) to the dqm's histogram's scale values (-4.9 to 4.9, example only)\n",
    "    tmp = hists_df[hists_df['paths']==hists_df['paths'].unique()[index_of_hist_of_interest]]\n",
    "    tmp_i = np.array([(idx) for idx,i in enumerate(range(int(tmp[tmp.columns[x_or_y_axis_as_0or1+1]].values.max()+1)))])\n",
    "    tmp_int = np.interp(tmp_i,(tmp_i.min(),tmp_i.max()),(-tmp[tmp.columns[x_or_y_axis_as_0or1+1]].values.max()/20,tmp[tmp.columns[x_or_y_axis_as_0or1+1]].values.max()/20))\n",
    "    tmp_int = tmp_int.round(2)\n",
    "    \n",
    "    # Prepare the conversion dictionary for this histogram ['bin_coordinate':dqm_scale_value]\n",
    "    dict_convertX = {}\n",
    "    for idx,val in enumerate(tmp_i):\n",
    "        for idxx,vall in enumerate(tmp_int):\n",
    "            if idx==idxx:\n",
    "                if x_or_y_axis_as_0or1 == 0:\n",
    "                    dict_convertX['x_'+str(val)] = vall\n",
    "                else:\n",
    "                    dict_convertX['y_'+str(val)] = vall\n",
    "\n",
    "    return dict_convertX\n",
    "\n",
    "#Example\n",
    "print('Example scale_cnvrt_dic to convert hitcoords from cern coordinates to bin coordinates (-1.8,1-.8)->(31,50)')\n",
    "print(scale_cnvrt_dic(pMain_hist20,0,0))\n",
    "\n",
    "print(f\"Example hitstring: 'Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:6829.0'\")\n",
    "\n",
    "def tenths_ceil(num_str):\n",
    "    \"\"\"\n",
    "    Currently, this gives the correct value when transforming (eta,phi)_cern coordinates to (eta,phi)_bin(the dataframe version of histograms) coordinates.\n",
    "    \n",
    "    This may not work in all cases, double check to make sure???\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get num_str as string\n",
    "    num_str = str(num_str)\n",
    "    \n",
    "    # Get num_str_fixed as num_str with the '-' removed if it exists\n",
    "    num_str_fixed = num_str.replace('-','')\n",
    "    \n",
    "    # Create the tuples of ints and decimals\n",
    "    int_tups = [('10e'+str(idx),char) for idx,char in enumerate(num_str_fixed.split(\".\")[0][::-1])]\n",
    "    dec_tups = [('10e'+str(-1*idx-1),char) for idx,char in enumerate(num_str_fixed.split(\".\")[1])]\n",
    "    \n",
    "    # Loop through decimal tuples\n",
    "    for id_,tup in enumerate(dec_tups):\n",
    "        \n",
    "        # Loop through each individual tuple's decimal value as it loops through tuples in decimals\n",
    "        for dec in tup[1]:\n",
    "            \n",
    "            # if its the first decimal, skip it\n",
    "            if id_ == 0:\n",
    "                continue\n",
    "                \n",
    "            # if any other decimals are greater than 0\n",
    "            if int(dec) > 0:\n",
    "                \n",
    "                # and if the given num_str input is negative, return the negative value truncated to the tenths\n",
    "                if '-' in num_str:  \n",
    "                    return -1 * ( float( ''.join( [tup[1] for tup in int_tups[::-1]] ) ) + float( '0.'+str ( int(dec_tups[0][1]) ) ) )\n",
    "                \n",
    "                # else if its not negative, return the postive value's ceiling to the tenths\n",
    "                else:\n",
    "                    return  float( ''.join( [tup[1] for tup in int_tups[::-1]] ) ) + float( '0.'+str ( int(dec_tups[0][1])+1 ) )\n",
    "                \n",
    "    # if none of the previous loops return a value, the num_str was not rounded up, so return the same negative value\n",
    "    if '-' in num_str:\n",
    "        return -1* ( float(''.join([tup[1] for tup in int_tups[::-1]]))+float('0.'+dec_tups[0][1]) )\n",
    "    \n",
    "    # else if none of the previous loops return a value, the num_str was not rounded up, so return the same positive value\n",
    "    else:        \n",
    "        return float(''.join([tup[1] for tup in int_tups[::-1]]))+float('0.'+dec_tups[0][1])\n",
    "\n",
    "print(f'Example tenths_ceil(-1.850,1.723): ({tenths_ceil(-1.850)},{tenths_ceil(1.723)})')\n",
    "    \n",
    "    \n",
    "def transform_hitstring(line):\n",
    "    \"\"\"\n",
    "    Takes a line in as the format received from the txt_file which was copy/pasted from cern's dqm display on a specific histogram, then extracts the (x_hitcoord,y_hitcoord)\n",
    "    \"\"\"\n",
    "    #line = \"Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:\t6829.0\"\n",
    "    \n",
    "    color_identifier_string=line[0]\n",
    "    \n",
    "    hit_number = line.replace(line[0],'').split('-',1)[0]\n",
    "    \n",
    "    occ_val = int(line.split(':')[1].replace('\\t','').split('.')[0])\n",
    "    \n",
    "    # Begin the process of extracting the x_hitcoord and y_hitcoord from line\n",
    "    line = line.replace(color_identifier_string+hit_number+'-'+'(eta,phi)[OSRatio]=','')\n",
    "    line = line.split(')')[0]\n",
    "    line = line.replace(\"(\",'')\n",
    "    line = line.split(',')\n",
    "    x_hitcoord, y_hitcoord = tenths_ceil( float( line[0] ) ), tenths_ceil( float ( line[1] ) )\n",
    "    \n",
    "    return ( x_hitcoord, y_hitcoord, color_identifier_string, hit_number, occ_val )\n",
    "\n",
    "# Example string, works as expected\n",
    "print(f'Example of transform_hitstring: {transform_hitstring(\"Y0-(eta,phi)[OSRatio]=(-1.850,1.723)[7.59e+01]:\t6829.0\")}')\n",
    "\n",
    "def extract_val_list(txt_file_path):\n",
    "    \"\"\"\n",
    "        \n",
    "    txt_file_path requires the pathname with the filename and file extension included.\n",
    "    Example: \"dir1/dir2/dir3/filename.txt\"\n",
    "    NOTE: Our current txt_file_path working directory is: \"../hist_targets_txt_files/\" such that the text file would look like \"../hist_targets_txt_files/filename.txt\"\n",
    "    \n",
    "    txt_file Structure:\n",
    "    It is a .txt file structured from a paste function after copying from the dqm starting from the line that reads \"NRedBins:\", then \"NYellowBins:\", then the lines of most interest.\n",
    "    From there, the lines of interest proceed line by line in a format such as the following ...\n",
    "    C#-(eta,phi)[OSRatio]=(x_coord,y_coord)[7.59e+01]: occ_val \n",
    "    where\n",
    "    C = color character identifier (Y for yellow bin, R for red bin)\n",
    "    # = the number of the R/Y bin (if there are NYellowBins=44 then this number will be a number from 0-44)\n",
    "    x_coord = the eta or x coordinate location of the point of interest (red or yellow hit)\n",
    "    y_coord = the phi or y coordinate location of the point of interest (red or yellow hit)\n",
    "    occ_Val = the occupancy value that was recorded at the location (x_coord,y_coord) \n",
    "    \n",
    "    val_list Structure:\n",
    "    It is a list of tuples whose tuples are each (x_hitcoord,y_hitcoord) such that ...\n",
    "    val_list = [ (xhc_0,yhc_0), (xhc_1,yhc_1), ..., (xhc_n,yhc_n) ]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Initialize the val_list\n",
    "    val_list = []\n",
    "    \n",
    "       \n",
    "    # Get a handle for the hist_path based on the txt_file_path ???\n",
    "    hist_path = txt_file_path.replace('-','/').replace('.txt','')\n",
    "    \n",
    "    # Open the text file\n",
    "    # directory_path = '../hist_targets_txt_files/'\n",
    "    with open('../hist_targets_txt_files/'+txt_file_path,\"r\") as f:       \n",
    "        # Extract the path_name(first line of the txt_file) - the first line is the path_name\n",
    "        path_name = f.readline()\n",
    "        \n",
    "        # Read the text file line by line and\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            # Skip these two lines\n",
    "            if 'NRedBins' in line:\n",
    "                continue\n",
    "            if 'NYellowBins' in line:\n",
    "                continue\n",
    "            \n",
    "            # Get a handle for the transform of the hitstring in each line of txt_file_path to the tuple of values of interest\n",
    "            transformed_line = transform_hitstring(line)\n",
    "            \n",
    "            # (x_hitcoord, y_hitcoord, color_identifier_string, hit_number, occ_val, hist_path)\n",
    "            val_list.append( ( transformed_line[0], transformed_line[1], transformed_line[2], transformed_line[3], transformed_line[4], hist_path ) )\n",
    "    \n",
    "    # return the val_list in the format as described above\n",
    "    return val_list\n",
    "\n",
    "# This example is very large and very long, turn on for debugging\n",
    "# print(f'Example extract_val_list using txt_file_path=\"run_348251-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt:')\n",
    "# display(extract_val_list(\"run_348251-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\"))\n",
    "    \n",
    "    \n",
    "\n",
    "def prep_quality_feature(list_of_hists_df, hist_index, txt_file_path):\n",
    "        \n",
    "    \"\"\"\n",
    "    val_list_xy must be structured as follows - it is a list of tuples whose tuples are each (x_hitcoord,y_hitcoord) such that \n",
    "    val_list_xy = [ (xhc_0,yhc_0), (xhc_1,yhc_1), ..., (xhc_n,yhc_n) ]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Convert the txt_file_path with '-'s in it to txt_file_path with '/'s in it instead\n",
    "#     txt_file_path2 = txt_file_path.replace('-','/')\n",
    "    \n",
    "    # Extract and get a handle for val_list from txt_file_path\n",
    "    val_list = extract_val_list(txt_file_path)\n",
    "    \n",
    "    # Convert our val_list tuple of 5 values to a tuple of 2 coordinate values (x,y)\n",
    "    val_list_xy = [(tup[0],tup[1]) for tup in val_list]\n",
    "    \n",
    "    # If the histogram of interest is in the list of paths that are associated with specific red/yellow hit coordinates (hit_n = (x,y,color,occ,hist_path))\n",
    "    if list_of_hists_df['paths'].unique()[hist_index] in [tup[5] for tup in val_list]:\n",
    "        \n",
    "        # Get a handle for the histogram we are constructing the quality feature for\n",
    "        tmp = list_of_hists_df[list_of_hists_df['paths']==list_of_hists_df['paths'].unique()[hist_index]]\n",
    "    else:\n",
    "        return \"Error: the unique histogram chosen as hist_index cannot be found for any of the hist_paths in val_list\"\n",
    "    \n",
    "    # Get the coordinate conversion dictionary\n",
    "    cnvrt_dic_x, cnvrt_dic_y = scale_cnvrt_dic(list_of_hists_df,hist_index,0), scale_cnvrt_dic(list_of_hists_df,hist_index,1)\n",
    "    \n",
    "    # Loop through the quality values, if the coordinates match the location of the hit, modify their quality value\n",
    "    for idx,val in enumerate(tmp['quality'].values):\n",
    "        \n",
    "        # If the tuple (x,y) from histogram tmp is in the list of (x,y) tuples from the hit value list (val_list_xy)\n",
    "        if (cnvrt_dic_x['x_'+str(int(tmp.iloc[idx,1]))],cnvrt_dic_y['y_'+str(int(tmp.iloc[idx,2]))]) in val_list_xy:\n",
    "            # Set the quality class for this hit (0/1 for green/red, 0/1/2 for green/yellow/red ...for now we just use 0/1)\n",
    "            tmp.iloc[idx,4] = 1 \n",
    "    \n",
    "    # Return the list_of_hists_df whose 'quality' values have been updated\n",
    "    return tmp\n",
    "\n",
    "# Example prep_quality_feature\n",
    "print(f'Example prep_quality_feature for pMain_hist20 dataframe, hist_index=0, txt_file_path = ... :')\n",
    "display(prep_quality_feature(pMain_hist20, 0, \"run_363710-CaloMonitoring-ClusterMon-CaloCalTopoClustersNoTrigSel-2d_Rates-m_clus_etaphi_Et_thresh1.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "personalized-testimony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T16:54:01.077871Z",
     "start_time": "2021-08-03T16:54:00.971689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the id value for the histogram that we are updating the target values\n",
    "hist_idx = 0\n",
    "\n",
    "# Set the string value for the path to the text file for the histogram we are updating\n",
    "txt_file_pth = \"\"\n",
    "\n",
    "# Set the database we are updating the target values to\n",
    "db = express_hist20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "shared-pasta",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T16:54:05.353493Z",
     "start_time": "2021-08-03T16:54:02.550867Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_100/x86_64-centos7-gcc8-opt/lib/python3.8/site-packages/pandas/core/indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "# Get a subset that does not include unique()[hist_index] == 0\n",
    "tmp = db[db['paths']!=db['paths'].unique()[0]]\n",
    "# Prepare the modified quality values\n",
    "tmp2 = prep_quality_feature(db, hist_idx, txt_file_pth)\n",
    "# then concatonate the pMain_hist20 with tmp_df\n",
    "pMain_hist20 = pd.concat([tmp,tmp2])\n",
    "\n",
    "# Save the databases\n",
    "express_hist20.to_csv(record_path+'express_hist20.csv')\n",
    "pMain_hist20.to_csv(record_path+'pMain_hist20.csv')\n",
    "\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-aircraft",
   "metadata": {},
   "source": [
    "checking results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "square-showcase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:01:12.090419Z",
     "start_time": "2021-08-03T17:01:11.982931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90090, 5)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "express_hist20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "boring-bridges",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:01:19.436241Z",
     "start_time": "2021-08-03T17:01:19.324176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    90090\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "express_hist20['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "taken-generator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T18:16:14.751132Z",
     "start_time": "2021-08-03T18:16:14.605769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['run_348251/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363664/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363710/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363738/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363830/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363910/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363947/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_363979/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364030/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364076/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364098/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364160/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364214/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1',\n",
       "       'run_364292/CaloMonitoring/ClusterMon/CaloCalTopoClustersNoTrigSel/2d_Rates/m_clus_etaphi_Et_thresh1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "express_hist20['paths'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-exploration",
   "metadata": {},
   "source": [
    "pMain_hist20 stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "daily-tampa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:06:16.708057Z",
     "start_time": "2021-08-03T17:06:16.600412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90090, 5)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pMain_hist20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "falling-drill",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:06:21.238560Z",
     "start_time": "2021-08-03T17:06:21.127868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    87489\n",
       "1     2601\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pMain_hist20['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "coral-surveillance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:06:23.928584Z",
     "start_time": "2021-08-03T17:06:23.810208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8871128871128873"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% bad to good values\n",
    "100*pMain_hist20['quality'].value_counts().values[1]/(pMain_hist20['quality'].value_counts().values[0]+pMain_hist20['quality'].value_counts().values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-exemption",
   "metadata": {},
   "source": [
    "##### Question for Dr. Sawyer:\n",
    "It appears that the express_express streams have very little errors, but it was suggested to me that express_express stream has many errors in it. It is possible that express_express contains many unknown errors and the physics_Main stream contains the same errors, but in this case they have been identified as known. On the other hand, it is possible that something is totally different between the two ...but what wouldnt make sense in this case is that the express_express then has many fewer ground_truth errors as yellow/red bins compared to the supposed to be more accurate physics_Main. What is the issue here?\n",
    "- response: use either generated data as suggested. Or continue to follow through with the pMain data as suggested and get more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-polyester",
   "metadata": {},
   "source": [
    "##### Official solution to above(8-4-21):\n",
    "According to the research I have done by the source below (1911), we can reconstruct a model similar to AnoNet and get good anomaly detection results on a single type of defect (dead cell/strip/layer) for as few as 1000 defect free images + 150 images with defect present.\n",
    "We will need to acquire approx...\n",
    "- 150 histograms with a single type of defect present (dead cell/strip/layer)\n",
    "- 1000 histograms with no defects present (look through pMain or express_express for histograms that are clean or do not have the defect present. confounding information could still be effective as long as the binary mask only highlights a single type of defect such as dead cell/strip/layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "differential-natural",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T22:59:57.469173Z",
     "start_time": "2021-08-04T22:59:57.465170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total histograms is 1150,\n",
      " pct histograms with a single type of defect present: 13.043478260869565%,\n",
      " pct histograms without any defects: 86.95652173913044%\n",
      "\n",
      " Note: this is not the percent of bad values to good values as we previously measured. Its the percent of histograms with a defect present\n"
     ]
    }
   ],
   "source": [
    "print(f\"total histograms is {1000+150},\\n pct histograms with a single type of defect present: {100*150/1150}%,\\n pct histograms without any defects: {100*1000/1150}%\")\n",
    "print(\"\\n Note: this is not the percent of bad values to good values as we previously measured. Its the percent of histograms with a defect present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-beatles",
   "metadata": {},
   "source": [
    "##### Use the following source to develop the CNN:\n",
    "- https://arxiv.org/pdf/1911.10608.pdf\n",
    "- For dilation mentioned in the paper -> https://en.wikipedia.org/wiki/Dilation_(morphology)\n",
    "- the first layer is a filter initialization bank layer...to setup your own custom filter look at the custom filter section of this: https://keras.io/api/layers/initializers/\n",
    "- then use the information here for how to add it to your first conv2d keras layer as described here: section 3.2 of .. https://arxiv.org/pdf/1911.10608.pdf\n",
    "- equation of ellipse for dilation(maybe useful?): https://stackoverflow.com/questions/10952060/plot-ellipse-with-matplotlib-pyplot-python\n",
    "- dilation over a numpy matrix: https://stackoverflow.com/questions/56735991/how-to-expand-dilate-a-numpy-array\n",
    "- creating your own activation function: https://arxiv.org/pdf/1502.01852.pdf (PReLU)\n",
    "- batch normalization: https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
    "- potentially better loss function: https://arxiv.org/pdf/1811.06861.pdf\n",
    "- more on loss functions for image restorations: https://arxiv.org/pdf/1511.08861.pdf\n",
    "- understanding binary cross entropy: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-vegetable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
